{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h1 style=\"padding: 10px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Transformacija podataka</h1>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h2 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Prikupljanje i predobrada podataka za Mašinsko učenje</h2>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h2 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Učitavanje biblioteka</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ucitavanje biblioteka\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,f1_score\n",
    "from sklearn.preprocessing import  LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop,Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h2 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Učitavanje datasetova</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ucitavanje datasetova\n",
    "airlineDataset = pd.read_csv(\"data/airlineData.csv\")\n",
    "bankDataset = pd.read_csv(\"data/bankData.csv\", sep=';')\n",
    "vehiclesDataset = pd.read_csv(\"data/vehiclesData.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h2 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Enkodiranje kategorickih atributa</h2>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h3 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Bank dataset</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enkodiranje output varijable\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(bankDataset[\"y\"])\n",
    "bankDataset[\"y\"] = label_encoder.transform(bankDataset[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kopiranje originalnog u pomocne datasetove\n",
    "bankDataset.dropna()\n",
    "bankDataset_MixEncoding = bankDataset.copy()\n",
    "bankDataset_LabelEncoding = bankDataset.copy()\n",
    "bankDataset_OneHotEncoding = bankDataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkOutliers_bank = bankDataset.select_dtypes(include=['int', 'float']).columns\n",
    "checkOutliers_bank = checkOutliers_bank.drop('y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provlacenje kategorickih atributa kroz one hot encoder\n",
    "cols = ['job', 'contact', 'marital', 'poutcome', 'month', 'day_of_week']\n",
    "bankDataset_MixEncoding = pd.get_dummies(data=bankDataset_MixEncoding, columns=cols)\n",
    "#bankDataset_MixEncoding.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoder \n",
    "labels = ['housing', 'default','education', 'loan']\n",
    "for label in labels:\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(bankDataset_MixEncoding[label])\n",
    "    bankDataset_MixEncoding[label] = label_encoder.transform(bankDataset_MixEncoding[label])\n",
    "#bankDataset_MixEncoding.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoder nad celim skupom\n",
    "kategoricki_atributi = bankDataset.select_dtypes(include='object').columns.tolist()\n",
    "for atribut in kategoricki_atributi:\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(bankDataset[atribut])\n",
    "    bankDataset_LabelEncoding[atribut] = label_encoder.transform(bankDataset_LabelEncoding[atribut])\n",
    "#bankDataset_LabelEncoding.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoder nad celim skupom\n",
    "bankDataset_OneHotEncoding = pd.get_dummies(data=bankDataset_OneHotEncoding,columns=kategoricki_atributi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankDataset_MixEncoding.nunique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h3 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Airline dataset</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enkodiranje output varijable\n",
    "airlineDataset = airlineDataset.dropna()\n",
    "\n",
    "from sklearn.preprocessing import  LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(airlineDataset[\"satisfaction\"])\n",
    "airlineDataset[\"satisfaction\"] = label_encoder.transform(airlineDataset[\"satisfaction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlineDataset.drop('id',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kopiranje originalnog dataseta u posebne datasetove za enkodiranje\n",
    "airlineDataset_MixEncoding = airlineDataset.copy()\n",
    "airlineDataset_OneHotEncoding = airlineDataset.copy()\n",
    "airlineDataset_LabelEncoding = airlineDataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkOutliers_airline = ['Age','Flight Distance','Departure Delay in Minutes','Arrival Delay in Minutes']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enkodiranje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode the 'Class' column\n",
    "class_map = {'Eco': 0, 'Eco Plus': 1, 'Business': 2}\n",
    "airlineDataset_MixEncoding['Class'] = airlineDataset_MixEncoding['Class'].replace(class_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode other atributes\n",
    "airlineDataset_MixEncoding = pd.get_dummies(airlineDataset_MixEncoding, columns=['Gender', 'Customer Type', 'Type of Travel'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Encoder nad celom skupu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_indexes = [0, 1, 3, 4] + list(range(6, 20))\n",
    "airlineDataset_LabelEncoding.iloc[:,categorical_indexes] = airlineDataset_LabelEncoding.iloc[:,categorical_indexes].astype('object')\n",
    "kategoricki_atributi_airline = airlineDataset_LabelEncoding.select_dtypes(include='object').columns.tolist()\n",
    "kategoricki_atributi_airline\n",
    "#airlineDataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Enkoder\n",
    "for atribut in kategoricki_atributi_airline:\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(airlineDataset_LabelEncoding[atribut])\n",
    "    airlineDataset_LabelEncoding[atribut] = label_encoder.transform(airlineDataset_LabelEncoding[atribut])\n",
    "    airlineDataset_LabelEncoding[atribut] = airlineDataset_LabelEncoding[atribut].astype('int')\n",
    "airlineDataset_LabelEncoding.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot Encoder na celom skupu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoder\n",
    "airlineDataset_OneHotEncoding = pd.get_dummies(data=airlineDataset_OneHotEncoding,columns=kategoricki_atributi_airline)\n",
    "airlineDataset_OneHotEncoding.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h3 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Primena algoritma</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pravimo dataframe koji ce cuvati rezultate algoritma za kasnije poredjenje\n",
    "testingDataframe = pd.DataFrame(columns=[\"Dataset\",\"Enkodiranje\", \"Algoritam\",\"F1\", \"Područje ispod ROC krive\", \"Accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_klasifikacija(df,output,Dataset,Enkodiranje,testingDataframe):\n",
    "    X = df.drop(output, axis=1)\n",
    "    y = df[output]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create an SVM classifier\n",
    "    svm = SVC(kernel='linear')\n",
    "\n",
    "    # Train the SVM classifier on the training data\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = svm.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy of the classifier\n",
    "    accuracy = accuracy_score(y_test, y_pred)*100\n",
    "    roc = roc_auc_score(y_test,y_pred)\n",
    "    f1 = f1_score(y_test,y_pred)\n",
    "\n",
    "    testingDataframe.loc[-1] = [Dataset, Enkodiranje, \"SVM\", f1, roc, '{:.2f}%'.format(accuracy)]\n",
    "    testingDataframe.index = testingDataframe.index + 1\n",
    "    testingDataframe = testingDataframe.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(df,output,Dataset,Enkodiranje,testingDataframe):\n",
    "    X = df.drop(output, axis=1)\n",
    "    y = df[output]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create a Naive Bayes classifier\n",
    "    nb = GaussianNB()\n",
    "\n",
    "    # Train the Naive Bayes classifier on the training data\n",
    "    nb.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = nb.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy of the classifier\n",
    "    accuracy = accuracy_score(y_test, y_pred)*100\n",
    "    roc = roc_auc_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    testingDataframe.loc[-1] = [Dataset, Enkodiranje, \"Naive Bayes\", f1, roc, '{:.2f}%'.format(accuracy)]\n",
    "    testingDataframe.index = testingDataframe.index + 1\n",
    "    testingDataframe = testingDataframe.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(df, output,Dataset, Enkodiranje,testingDataframe):\n",
    "    X = df.drop(output, axis=1)\n",
    "    y = df[output]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create a Random Forest classifier\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    # Train the Random Forest classifier on the training data\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy of the classifier\n",
    "    accuracy = accuracy_score(y_test, y_pred)*100\n",
    "    roc = roc_auc_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    testingDataframe.loc[-1] = [Dataset, Enkodiranje, \"Random Forest\", f1, roc, '{:.2f}%'.format(accuracy)]\n",
    "    testingDataframe.index = testingDataframe.index + 1\n",
    "    testingDataframe = testingDataframe.sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(df, output,dataset,enkodiranje,testingDataframe):\n",
    "    X = df.drop(output, axis=1)\n",
    "    y = df[output]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "    # Set up early stopping to prevent overfitting\n",
    "    es = EarlyStopping(monitor='val_accuracy', mode='max', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    NN_Classifier = model.fit(X_train, y_train, batch_size=1000, epochs=1000, callbacks=[es], verbose=2, validation_data=(X_test, y_test))\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = model.predict(X_test).round().astype(int)\n",
    "\n",
    "    # Calculate the accuracy, AUC, and F1 score of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)*100\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Add the model performance metrics to the testingDataframe\n",
    "    testingDataframe.loc[-1] = [dataset, enkodiranje, \"Neural network\", f1, auc, '{:.2f}%'.format(accuracy)]\n",
    "    testingDataframe.index = testingDataframe.index + 1\n",
    "    testingDataframe = testingDataframe.sort_index()\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h3 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Bank dataset</h3>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h4 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "SVM</h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_klasifikacija(bankDataset_MixEncoding,'y',\"Bank\",\"MIX\",testingDataframe)\n",
    "svm_klasifikacija(bankDataset_LabelEncoding,'y',\"Bank\",\"LabelEncoding\",testingDataframe)\n",
    "svm_klasifikacija(bankDataset_OneHotEncoding,'y',\"Bank\",\"OneHotEncoding\",testingDataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingDataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM on Mixed Encodings Bank dataset\n",
    "# Extract the features and target variable\n",
    "X_Bank_Mix = bankDataset_MixEncoding.drop('y', axis=1)\n",
    "y_Bank_Mix = bankDataset_MixEncoding['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_BM, X_test_BM, y_train_BM, y_test_BM = train_test_split(X_Bank_Mix, y_Bank_Mix, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "# Train the SVM classifier on the training data\n",
    "svm.fit(X_train_BM, y_train_BM)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred_BM = svm.predict(X_test_BM)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test_BM, y_pred_BM)*100\n",
    "roc = roc_auc_score(y_test_BM,y_pred_BM)\n",
    "f1 = f1_score(y_test_BM,y_pred_BM)\n",
    "\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank dataset\",\"MIX\",\"SVM\",f1,  roc, '{:.2f}%'.format(accuracy)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM on All Label Encoded Bank dataset\n",
    "# Extract the features and target variable\n",
    "X_Bank_LE = bankDataset_LabelEncoding.drop('y', axis=1)\n",
    "y_Bank_LE = bankDataset_LabelEncoding['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_BLE, X_test_BLE, y_train_BLE, y_test_BLE = train_test_split(X_Bank_LE, y_Bank_LE, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "# Train the SVM classifier on the training data\n",
    "svm.fit(X_train_BLE, y_train_BLE)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred_BLE = svm.predict(X_test_BLE)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test_BLE, y_pred_BLE)*100\n",
    "roc = roc_auc_score(y_test_BLE,y_pred_BLE)\n",
    "f1 = f1_score(y_test_BLE,y_pred_BLE)\n",
    "\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank dataset\",\"Label Encoder\",\"SVM\",f1,  roc,  '{:.2f}%'.format(accuracy)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM on All Hot Encoded Bank dataset\n",
    "# Extract the features and target variable\n",
    "X_Bank_OH = bankDataset_OneHotEncoding.drop(['y'], axis=1)\n",
    "y_Bank_OH = bankDataset_OneHotEncoding['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_BOH, X_test_BOH, y_train_BOH, y_test_BOH = train_test_split(X_Bank_OH, y_Bank_OH, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "# Train the SVM classifier on the training data\n",
    "svm.fit(X_train_BOH, y_train_BOH)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred_BOH = svm.predict(X_test_BOH)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test_BOH, y_pred_BOH)*100\n",
    "roc = roc_auc_score(y_test_BOH,y_pred_BOH)\n",
    "f1 = f1_score(y_test_BOH,y_pred_BOH)\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank dataset\",\"One Hot\",\"SVM\",f1,  roc,  '{:.2f}%'.format(accuracy)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h4 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Naive Bayes</h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes(bankDataset_MixEncoding,'y',\"Bank\",\"MIX\",testingDataframe)\n",
    "naive_bayes(bankDataset_LabelEncoding,'y',\"Bank\",\"LabelEncoding\",testingDataframe)\n",
    "naive_bayes(bankDataset_OneHotEncoding,'y',\"Bank\",\"OneHotEncoding\",testingDataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingDataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NaiveBayes on Mixed Encodings Bank dataset\n",
    "# Extract the features and target variable\n",
    "X_Bank_Mix_NB = bankDataset_MixEncoding.drop('y', axis=1)\n",
    "y_Bank_Mix_NB = bankDataset_MixEncoding['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_BM_NB, X_test_BM_NB, y_train_BM_NB, y_test_BM_NB = train_test_split(X_Bank_Mix_NB, y_Bank_Mix_NB, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Naive Bayes classifier\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Train the Naive Bayes classifier on the training data\n",
    "nb.fit(X_train_BM_NB, y_train_BM_NB)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred_BM_NB = nb.predict(X_test_BM_NB)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test_BM_NB, y_pred_BM_NB)*100\n",
    "roc = roc_auc_score(y_test_BM_NB, y_pred_BM_NB)\n",
    "f1 = f1_score(y_test_BM_NB, y_pred_BM_NB)\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank dataset\",\"MIX\",\"Naive Bayes\",f1,  roc,  '{:.2f}%'.format(accuracy)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaiveBayes on All Label Encoded Bank dataset\n",
    "#  Extract the features and target variable\n",
    "X_Bank_LE_NB = bankDataset_LabelEncoding.drop('y', axis=1)\n",
    "y_Bank_LE_NB = bankDataset_LabelEncoding['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_BLE_NB, X_test_BLE_NB, y_train_BLE_NB, y_test_BLE_NB = train_test_split(X_Bank_LE_NB, y_Bank_LE_NB, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Naive Bayes classifier\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Train the Naive Bayes classifier on the training data\n",
    "nb.fit(X_train_BLE_NB, y_train_BLE_NB)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred_BLE_NB = nb.predict(X_test_BLE_NB)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test_BLE_NB, y_pred_BLE_NB)*100\n",
    "roc = roc_auc_score(y_test_BLE_NB, y_pred_BLE_NB)\n",
    "f1 = f1_score(y_test_BLE_NB, y_pred_BLE_NB)\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank dataset\",\"Label Encoder\",\"Naive Bayes\",f1,  roc,  '{:.2f}%'.format(accuracy)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaiveBayes on All One Hot Encodings Bank dataset\n",
    "# Extract the features and target variable\n",
    "X_Bank_OH_NB = bankDataset_OneHotEncoding.drop(['y'], axis=1)\n",
    "y_Bank_OH_NB = bankDataset_OneHotEncoding['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_BOH_NB, X_test_BOH_NB, y_train_BOH_NB, y_test_BOH_NB = train_test_split(X_Bank_OH_NB, y_Bank_OH_NB, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Naive Bayes classifier\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Train the Naive Bayes classifier on the training data\n",
    "nb.fit(X_train_BOH_NB, y_train_BOH_NB)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred_BOH_NB = nb.predict(X_test_BOH_NB)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test_BOH_NB, y_pred_BOH_NB)*100\n",
    "roc = roc_auc_score(y_test_BOH_NB, y_pred_BOH_NB)\n",
    "f1 = f1_score(y_test_BOH_NB, y_pred_BOH_NB)\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank dataset\",\"One Hot\",\"Naive Bayes\",f1, roc, '{:.2f}%'.format(accuracy)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h4 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Random Forest</h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest(bankDataset_MixEncoding,'y',\"Bank\",\"MIX\",testingDataframe)\n",
    "random_forest(bankDataset_LabelEncoding,'y',\"Bank\",\"LabelEncoding\",testingDataframe)\n",
    "random_forest(bankDataset_OneHotEncoding,'y',\"Bank\",\"OneHotEncoding\",testingDataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingDataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest on Mixed Encoded Bank dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "# Extract the features and target variable\n",
    "X_Bank_Mix_RF = bankDataset_MixEncoding.drop(['y'], axis=1)\n",
    "y_Bank_Mix_RF = bankDataset_MixEncoding['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_BM_RF, X_test_BM_RF, y_train_BM_RF, y_test_BM_RF = train_test_split(X_Bank_Mix_RF, y_Bank_Mix_RF, test_size=0.2, random_state=42)\n",
    "\n",
    "randomForest_Bank_Mix = RandomForestClassifier()\n",
    "randomForest_Bank_Mix.fit(X_train_BM_RF, y_train_BM_RF)\n",
    "\n",
    "pred = randomForest_Bank_Mix.predict(X_test_BM_RF)\n",
    "f1 = f1_score(y_test_BM_RF, randomForest_Bank_Mix.predict(X_test_BM_RF))\n",
    "auc = roc_auc_score(y_test_BM_RF, randomForest_Bank_Mix.predict(X_test_BM_RF))\n",
    "acc = accuracy_score(y_test_BM_RF, randomForest_Bank_Mix.predict(X_test_BM_RF))*100\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank dataset\",\"MIX\",\"Random forest\",f1,  auc,  '{:.2f}%'.format(acc)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest on All Label Encoded Bank dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "# Extract the features and target variable\n",
    "X_Bank_LE_RF = bankDataset_LabelEncoding.drop(['y'], axis=1)\n",
    "y_Bank_LE_RF = bankDataset_LabelEncoding['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_LE_RF, X_test_LE_RF, y_train_LE_RF, y_test_LE_RF = train_test_split(X_Bank_LE_RF, y_Bank_LE_RF, test_size=0.2, random_state=42)\n",
    "\n",
    "randomForest_Bank_LE = RandomForestClassifier()\n",
    "randomForest_Bank_LE.fit(X_train_LE_RF, y_train_LE_RF)\n",
    "\n",
    "pred = randomForest_Bank_LE.predict(X_test_LE_RF)\n",
    "f1 = f1_score(y_test_LE_RF, randomForest_Bank_LE.predict(X_test_LE_RF))\n",
    "auc = roc_auc_score(y_test_LE_RF, randomForest_Bank_LE.predict(X_test_LE_RF))\n",
    "acc = accuracy_score(y_test_LE_RF, randomForest_Bank_LE.predict(X_test_LE_RF))*100\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank dataset\",\"Label Encoder\",\"Random forest\",f1,  auc,  '{:.2f}%'.format(acc)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest on All One Hot Encoded Bank dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "# Extract the features and target variable\n",
    "X_Bank_BOH_RF = bankDataset_OneHotEncoding.drop(['y'], axis=1)\n",
    "y_Bank_BOH_RF = bankDataset_OneHotEncoding['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_BOH_RF, X_test_BOH_RF, y_train_BOH_RF, y_test_BOH_RF = train_test_split(X_Bank_BOH_RF, y_Bank_BOH_RF, test_size=0.2, random_state=42)\n",
    "\n",
    "randomForest_Bank_LE = RandomForestClassifier()\n",
    "randomForest_Bank_LE.fit(X_train_BOH_RF, y_train_BOH_RF)\n",
    "\n",
    "pred = randomForest_Bank_LE.predict(X_test_BOH_RF)\n",
    "f1 = f1_score(y_test_BOH_RF, randomForest_Bank_LE.predict(X_test_BOH_RF))\n",
    "auc = roc_auc_score(y_test_BOH_RF, randomForest_Bank_LE.predict(X_test_BOH_RF))\n",
    "acc = accuracy_score(y_test_BOH_RF, randomForest_Bank_LE.predict(X_test_BOH_RF))*100\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank dataset\",\"One Hot\",\"Random forest\",f1,  auc,  '{:.2f}%'.format(acc)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h4 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Artificial Neural Networks</h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network(bankDataset_MixEncoding,'y',\"Bank\",\"MIX\",testingDataframe)\n",
    "neural_network(bankDataset_LabelEncoding,'y',\"Bank\",\"LabelEncoding\",testingDataframe)\n",
    "neural_network(bankDataset_OneHotEncoding,'y',\"Bank\",\"OneHotEncoding\",testingDataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingDataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN - Mixed Encoding - Bank dataset\n",
    "X_Bank_BM_ANN = bankDataset_MixEncoding.drop(['y'], axis=1)\n",
    "y_Bank_BM_ANN = bankDataset_MixEncoding['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_BM_ANN, X_test_BM_ANN, y_train_BM_ANN, y_test_BM_ANN = train_test_split(X_Bank_BM_ANN, y_Bank_BM_ANN, test_size=0.2, random_state=42)\n",
    "\n",
    "model_BM_ANN = Sequential()\n",
    "model_BM_ANN.add(Dense(512, activation='relu', input_shape=(X_train_BM_ANN.shape[1],)))\n",
    "model_BM_ANN.add(Dropout(0.3))\n",
    "model_BM_ANN.add(Dense(256, activation='relu'))\n",
    "model_BM_ANN.add(Dense(256, activation='relu'))\n",
    "model_BM_ANN.add(Dropout(0.3))\n",
    "model_BM_ANN.add(Dense(128, activation='relu'))\n",
    "model_BM_ANN.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model_BM_ANN.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', \n",
    "                   mode='max', \n",
    "                   patience=10,  #Stop the model training if the validation accuracy doesnt increase in 10 Epochs\n",
    "                   restore_best_weights=True)\n",
    "\n",
    "NN_Classifier_BM = model_BM_ANN.fit(X_train_BM_ANN,y_train_BM_ANN,\n",
    "                          batch_size=1000,\n",
    "                          epochs=1000,\n",
    "                          callbacks =[es],\n",
    "                          verbose=2,\n",
    "                          validation_data=(X_test_BM_ANN, y_test_BM_ANN))\n",
    "                          \n",
    "\n",
    "y_pred=(model_BM_ANN.predict(X_test_BM_ANN) > 0.5).astype(int)\n",
    "acc = accuracy_score(y_test_BM_ANN, y_pred)*100\n",
    "#print('Accuracy Score: ', '{:.2f}%'.format(acc))\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank dataset\",\"MIX\",\"ANN\",0,  0, '{:.2f}%'.format(acc)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN - Label Encoding -  Bank dataset\n",
    "X_Bank_BLE_ANN = bankDataset_LabelEncoding.drop(['y'], axis=1)\n",
    "y_Bank_BLE_ANN = bankDataset_LabelEncoding['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_BLE_ANN, X_test_BLE_ANN, y_train_BLE_ANN, y_test_BLE_ANN = train_test_split(X_Bank_BLE_ANN, y_Bank_BLE_ANN, test_size=0.2, random_state=42)\n",
    "\n",
    "model_BLE_ANN = Sequential()\n",
    "model_BLE_ANN.add(Dense(512, activation='relu', input_shape=(X_train_BLE_ANN.shape[1],)))\n",
    "model_BLE_ANN.add(Dropout(0.3))\n",
    "model_BLE_ANN.add(Dense(256, activation='relu'))\n",
    "model_BLE_ANN.add(Dense(256, activation='relu'))\n",
    "model_BLE_ANN.add(Dropout(0.3))\n",
    "model_BLE_ANN.add(Dense(128, activation='relu'))\n",
    "model_BLE_ANN.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model_BLE_ANN.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', \n",
    "                   mode='max', \n",
    "                   patience=10,  #Stop the model training if the validation accuracy doesnt increase in 10 Epochs\n",
    "                   restore_best_weights=True)\n",
    "\n",
    "NN_Classifier_BLE = model_BLE_ANN.fit(X_train_BLE_ANN, y_train_BLE_ANN,\n",
    "                          batch_size=1000,\n",
    "                          epochs=1000,\n",
    "                          callbacks =[es],\n",
    "                          verbose=2,\n",
    "                          validation_data=(X_test_BLE_ANN, y_test_BLE_ANN))\n",
    "\n",
    "\n",
    "y_pred=(model_BLE_ANN.predict(X_test_BLE_ANN) > 0.5).astype(int)\n",
    "acc = accuracy_score(y_test_BLE_ANN, y_pred)*100\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank dataset\",\"Label Encoder\",\"ANN\",0,  0,  '{:.2f}%'.format(acc)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN - One Hot Encoding -  Bank dataset\n",
    "X_Bank_BOH_ANN = bankDataset_OneHotEncoding.drop(['y'], axis=1)\n",
    "y_Bank_BOH_ANN = bankDataset_OneHotEncoding['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_BOH_ANN, X_test_BOH_ANN, y_train_BOH_ANN, y_test_BOH_ANN = train_test_split(X_Bank_BOH_ANN, y_Bank_BOH_ANN, test_size=0.2, random_state=42)\n",
    "\n",
    "model_BOH_ANN = Sequential()\n",
    "model_BOH_ANN.add(Dense(512, activation='relu', input_shape=(X_train_BOH_ANN.shape[1],)))\n",
    "model_BOH_ANN.add(Dropout(0.3))\n",
    "model_BOH_ANN.add(Dense(256, activation='relu'))\n",
    "model_BOH_ANN.add(Dense(256, activation='relu'))\n",
    "model_BOH_ANN.add(Dropout(0.3))\n",
    "model_BOH_ANN.add(Dense(128, activation='relu'))\n",
    "model_BOH_ANN.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model_BOH_ANN.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', \n",
    "                   mode='max', \n",
    "                   patience=10,  #Stop the model training if the validation accuracy doesnt increase in 10 Epochs\n",
    "                   restore_best_weights=True)\n",
    "\n",
    "NN_Classifier_BOH = model_BOH_ANN.fit(X_train_BOH_ANN, y_train_BOH_ANN,\n",
    "                          batch_size=1000,\n",
    "                          epochs=1000,\n",
    "                          callbacks =[es],\n",
    "                          verbose=2,\n",
    "                          validation_data=(X_test_BOH_ANN, y_test_BOH_ANN))\n",
    "\n",
    "\n",
    "y_pred=(model_BOH_ANN.predict(X_test_BOH_ANN) > 0.5).astype(int)\n",
    "acc = accuracy_score(y_test_BOH_ANN, y_pred)*100\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank dataset\",\"One Hot\",\"ANN\",0,  0,  '{:.2f}%'.format(acc)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h3 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Airline dataset</h3>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h4 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Naive bayes</h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes(airlineDataset_MixEncoding,'satisfaction',\"Airline\",\"MIX\",testingDataframe)\n",
    "naive_bayes(airlineDataset_LabelEncoding,'satisfaction',\"Airline\",\"LabelEncoding\",testingDataframe)\n",
    "naive_bayes(airlineDataset_OneHotEncoding,'satisfaction',\"Airline\",\"OneHotEncoding\",testingDataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingDataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaiveBayes - Mixed Encoding - Airline dataset\n",
    "\n",
    "X_Airline_Mix_NB = airlineDataset_MixEncoding.drop('satisfaction', axis=1)\n",
    "y_Airline_Mix_NB = airlineDataset_MixEncoding['satisfaction']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_AM_NB, X_test_AM_NB, y_train_AM_NB, y_test_AM_NB = train_test_split(X_Airline_Mix_NB, y_Airline_Mix_NB, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Naive Bayes classifier\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Train the Naive Bayes classifier on the training data\n",
    "nb.fit(X_train_AM_NB, y_train_AM_NB)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred_AM_NB = nb.predict(X_test_AM_NB)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test_AM_NB, y_pred_AM_NB)*100\n",
    "roc = roc_auc_score(y_test_AM_NB, y_pred_AM_NB)\n",
    "f1 = f1_score(y_test_AM_NB, y_pred_AM_NB)\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Airline dataset\",\"MIX\",\"Naive Bayes\",f1,  roc,  '{:.2f}%'.format(accuracy)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaiveBayes - Label Encoding - Airline dataset\n",
    "\n",
    "# Extract the features and target variable\n",
    "X_Airline_LE_NB = airlineDataset_LabelEncoding.drop('satisfaction', axis=1)\n",
    "y_Airline_LE_NB = airlineDataset_LabelEncoding['satisfaction']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_ALE_NB, X_test_ALE_NB, y_train_ALE_NB, y_test_ALE_NB = train_test_split(X_Airline_LE_NB, y_Airline_LE_NB, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Naive Bayes classifier\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Train the Naive Bayes classifier on the training data\n",
    "nb.fit(X_train_ALE_NB, y_train_ALE_NB)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred_ALE_NB = nb.predict(X_test_ALE_NB)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test_ALE_NB, y_pred_ALE_NB)*100\n",
    "roc = roc_auc_score(y_test_ALE_NB, y_pred_ALE_NB)\n",
    "f1 = f1_score(y_test_ALE_NB, y_pred_ALE_NB)\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Airline dataset\",\"Label Encoder\",\"Naive Bayes\",f1,  roc,  '{:.2f}%'.format(accuracy)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaiveBayes - One Hot Encoding - Airline dataset\n",
    "\n",
    "# Extract the features and target variable\n",
    "X_Airline_OH_NB = airlineDataset_OneHotEncoding.drop(['satisfaction'], axis=1)\n",
    "y_Airline_OH_NB = airlineDataset_OneHotEncoding['satisfaction']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_AOH_NB, X_test_AOH_NB, y_train_AOH_NB, y_test_AOH_NB = train_test_split(X_Airline_OH_NB, y_Airline_OH_NB, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Naive Bayes classifier\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Train the Naive Bayes classifier on the training data\n",
    "nb.fit(X_train_AOH_NB, y_train_AOH_NB)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred_AOH_NB = nb.predict(X_test_AOH_NB)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test_AOH_NB, y_pred_AOH_NB)*100\n",
    "roc = roc_auc_score(y_test_AOH_NB, y_pred_AOH_NB)\n",
    "f1 = f1_score(y_test_AOH_NB, y_pred_AOH_NB)\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Airline dataset\",\"One Hot\",\"Naive Bayes\",f1, roc, '{:.2f}%'.format(accuracy)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h4 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Random Forest</h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest(airlineDataset_MixEncoding,'satisfaction',\"Airline\",\"MIX\",testingDataframe)\n",
    "random_forest(airlineDataset_LabelEncoding,'satisfaction',\"Airline\",\"LabelEncoding\",testingDataframe)\n",
    "random_forest(airlineDataset_OneHotEncoding,'satisfaction',\"Airline\",\"OneHotEncoding\",testingDataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest - Mixed Encoding - Airline dataset\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Extract the features and target variable\n",
    "X_Airline_Mix_RF = airlineDataset_MixEncoding.drop(['satisfaction'], axis=1)\n",
    "y_Airline_Mix_RF = airlineDataset_MixEncoding['satisfaction']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_AM_RF, X_test_AM_RF, y_train_AM_RF, y_test_AM_RF = train_test_split(X_Airline_Mix_RF, y_Airline_Mix_RF, test_size=0.2, random_state=42)\n",
    "\n",
    "randomForest_Airline_Mix = RandomForestClassifier()\n",
    "randomForest_Airline_Mix.fit(X_train_AM_RF, y_train_AM_RF)\n",
    "\n",
    "pred = randomForest_Airline_Mix.predict(X_test_AM_RF)\n",
    "f1 = f1_score(y_test_AM_RF, randomForest_Airline_Mix.predict(X_test_AM_RF))\n",
    "auc = roc_auc_score(y_test_AM_RF, randomForest_Airline_Mix.predict(X_test_AM_RF))\n",
    "acc = accuracy_score(y_test_AM_RF, randomForest_Airline_Mix.predict(X_test_AM_RF))*100\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Airline dataset\",\"MIX\",\"Random forest\",f1,  auc,  '{:.2f}%'.format(acc)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest - Label Encoding - Airline dataset\n",
    "\n",
    "\n",
    "# Extract the features and target variable\n",
    "X_Airline_LE_RF = airlineDataset_LabelEncoding.drop(['satisfaction'], axis=1)\n",
    "y_Airline_LE_RF = airlineDataset_LabelEncoding['satisfaction']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_ALE_RF, X_test_ALE_RF, y_train_ALE_RF, y_test_ALE_RF = train_test_split(X_Airline_LE_RF, y_Airline_LE_RF, test_size=0.2, random_state=42)\n",
    "\n",
    "randomForest_Airline_LE = RandomForestClassifier()\n",
    "randomForest_Airline_LE.fit(X_train_ALE_RF, y_train_ALE_RF)\n",
    "\n",
    "pred = randomForest_Airline_LE.predict(X_test_ALE_RF)\n",
    "f1 = f1_score(y_test_ALE_RF, randomForest_Airline_LE.predict(X_test_ALE_RF))\n",
    "auc = roc_auc_score(y_test_ALE_RF, randomForest_Airline_LE.predict(X_test_ALE_RF))\n",
    "acc = accuracy_score(y_test_ALE_RF, randomForest_Airline_LE.predict(X_test_ALE_RF))*100\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Airline dataset\",\"Label Encoder\",\"Random forest\",f1,  auc,  '{:.2f}%'.format(acc)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest - OneHot Encoding - Airline dataset\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Extract the features and target variable\n",
    "X_Airline_BOH_RF = airlineDataset_OneHotEncoding.drop(['satisfaction'], axis=1)\n",
    "y_Airline_BOH_RF = airlineDataset_OneHotEncoding['satisfaction']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_AOH_RF, X_test_AOH_RF, y_train_AOH_RF, y_test_AOH_RF = train_test_split(X_Airline_BOH_RF, y_Airline_BOH_RF, test_size=0.2, random_state=42)\n",
    "\n",
    "randomForest_Airline_OH = RandomForestClassifier()\n",
    "randomForest_Airline_OH.fit(X_train_AOH_RF, y_train_AOH_RF)\n",
    "\n",
    "pred = randomForest_Airline_OH.predict(X_test_AOH_RF)\n",
    "f1 = f1_score(y_test_AOH_RF, randomForest_Airline_OH.predict(X_test_AOH_RF))\n",
    "auc = roc_auc_score(y_test_AOH_RF, randomForest_Airline_OH.predict(X_test_AOH_RF))\n",
    "acc = accuracy_score(y_test_AOH_RF, randomForest_Airline_OH.predict(X_test_AOH_RF))*100\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Airline dataset\",\"One Hot\",\"Random forest\",f1,  auc,  '{:.2f}%'.format(acc)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h4 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Artificial Neural Network</h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network(airlineDataset_MixEncoding,'satisfaction',\"Airline\",\"MIX\",testingDataframe)\n",
    "neural_network(airlineDataset_LabelEncoding,'satisfaction',\"Airline\",\"LabelEncoding\",testingDataframe)\n",
    "neural_network(airlineDataset_OneHotEncoding,'satisfaction',\"Airline\",\"OneHotEncoding\",testingDataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingDataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN - Mixed Encoding - Airline dataset\n",
    "\n",
    "def naive_bayes(df,output):\n",
    "    X = df.drop(output, axis=1)\n",
    "    y = df[output]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create a Naive Bayes classifier\n",
    "    nb = GaussianNB()\n",
    "\n",
    "    # Train the Naive Bayes classifier on the training data\n",
    "    nb.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = nb.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy of the classifier\n",
    "    accuracy = accuracy_score(y_test, y_pred)*100\n",
    "    roc = roc_auc_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "#print('Accuracy Score: ', '{:.2f}%'.format(acc))\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Airline dataset\",\"MIX\",\"ANN\",0,  0, '{:.2f}%'.format(acc)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN - Label Encoding - Airline dataset\n",
    "\n",
    "\n",
    "X_Airline_BLE_ANN = airlineDataset_LabelEncoding.drop(['satisfaction'], axis=1)\n",
    "y_Airline_BLE_ANN = airlineDataset_LabelEncoding['satisfaction']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_ALE_ANN, X_test_ALE_ANN, y_train_ALE_ANN, y_test_ALE_ANN = train_test_split(X_Airline_BLE_ANN, y_Airline_BLE_ANN, test_size=0.2, random_state=42)\n",
    "\n",
    "model_ALE_ANN = Sequential()\n",
    "model_ALE_ANN.add(Dense(512, activation='relu', input_shape=(X_train_ALE_ANN.shape[1],)))\n",
    "model_ALE_ANN.add(Dropout(0.3))\n",
    "model_ALE_ANN.add(Dense(256, activation='relu'))\n",
    "model_ALE_ANN.add(Dense(256, activation='relu'))\n",
    "model_ALE_ANN.add(Dropout(0.3))\n",
    "model_ALE_ANN.add(Dense(128, activation='relu'))\n",
    "model_ALE_ANN.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model_ALE_ANN.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', \n",
    "                   mode='max', \n",
    "                   patience=10,  #Stop the model training if the validation accuracy doesnt increase in 10 Epochs\n",
    "                   restore_best_weights=True)\n",
    "\n",
    "NN_Classifier_ALE = model_ALE_ANN.fit(X_train_ALE_ANN, y_train_ALE_ANN,\n",
    "                          batch_size=1000,\n",
    "                          epochs=1000,\n",
    "                          callbacks =[es],\n",
    "                          verbose=2,\n",
    "                          validation_data=(X_test_ALE_ANN, y_test_ALE_ANN))\n",
    "\n",
    "\n",
    "y_pred=(model_ALE_ANN.predict(X_test_ALE_ANN) > 0.5).astype(int)\n",
    "acc = accuracy_score(y_test_ALE_ANN, y_pred)*100\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Airline dataset\",\"Label Encoder\",\"ANN\",0,  0,  '{:.2f}%'.format(acc)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN - OneHot Encoding - Airline dataset\n",
    "\n",
    "X_Airline_AOH_ANN = airlineDataset_OneHotEncoding.drop(['satisfaction'], axis=1)\n",
    "y_Airline_AOH_ANN = airlineDataset_OneHotEncoding['satisfaction']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_AOH_ANN, X_test_AOH_ANN, y_train_AOH_ANN, y_test_AOH_ANN = train_test_split(X_Airline_AOH_ANN, y_Airline_AOH_ANN, test_size=0.2, random_state=42)\n",
    "\n",
    "model_AOH_ANN = Sequential()\n",
    "model_AOH_ANN.add(Dense(512, activation='relu', input_shape=(X_train_AOH_ANN.shape[1],)))\n",
    "model_AOH_ANN.add(Dropout(0.3))\n",
    "model_AOH_ANN.add(Dense(256, activation='relu'))\n",
    "model_AOH_ANN.add(Dense(256, activation='relu'))\n",
    "model_AOH_ANN.add(Dropout(0.3))\n",
    "model_AOH_ANN.add(Dense(128, activation='relu'))\n",
    "model_AOH_ANN.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model_AOH_ANN.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', \n",
    "                   mode='max', \n",
    "                   patience=10,  #Stop the model training if the validation accuracy doesnt increase in 10 Epochs\n",
    "                   restore_best_weights=True)\n",
    "\n",
    "NN_Classifier_AOH = model_AOH_ANN.fit(X_train_AOH_ANN, y_train_AOH_ANN,\n",
    "                          batch_size=1000,\n",
    "                          epochs=1000,\n",
    "                          callbacks =[es],\n",
    "                          verbose=2,\n",
    "                          validation_data=(X_test_AOH_ANN, y_test_AOH_ANN))\n",
    "\n",
    "\n",
    "y_pred=(model_AOH_ANN.predict(X_test_AOH_ANN) > 0.5).astype(int)\n",
    "acc = accuracy_score(y_test_AOH_ANN, y_pred)*100\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Airline dataset\",\"One Hot\",\"ANN\",0,  0,  '{:.2f}%'.format(acc)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h3 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Poredjenje rezultata</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting by Accuracy\n",
    "testing = testingDataframe.sort_values(by=['Accuracy'],ascending=False)\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting by F1\n",
    "testing = testingDataframe.sort_values(by=['F1'],ascending=False)\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting by ROC\n",
    "testing = testingDataframe.sort_values(by=['Područje ispod ROC krive'],ascending=False)\n",
    "testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h2 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Skaliranje podataka</h2>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: left;\n",
    "            padding-left: 20px\">\n",
    "            \n",
    "<p style=\"padding: 5px 0px; color:#2980B9; font-family: Arial, sans-serif;\">\n",
    "U teorijskom delu rada, bilo je reči o razlikama između standardizacije i normalizacije podataka. Predstavljene su dobre i loše strane kao i tehnike koje se koriste. Napravljena je podela i rečeno je da su tehnike koje se korista za normalizaciju: <br>\n",
    "    1. MinMax Scaler<br>\n",
    "    2. MaxAbs Scaler<br>\n",
    "    3. Robust Scaler<br>\n",
    "A tehnika koja se koristi za standardizaciju:<br>\n",
    "    <t>1. Standard Scaler<br></p>\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h3 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Tehnike normalizacije podataka</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizovanje podataka MinMaxScaler-om\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def normalize_data(df,output):\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_df = (\n",
    "        df.drop(columns=[output])\n",
    "        .pipe(lambda df: pd.DataFrame(scaler.fit_transform(df), columns=df.columns))\n",
    "        .assign(y=df[output])\n",
    "    )\n",
    "    return normalized_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizovanje podataka MaxAbsScaler-om\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "def normalize_data_maxabs(df,output):\n",
    "    scaler = MaxAbsScaler()\n",
    "    normalized_df = (\n",
    "        df.drop(columns=[output])\n",
    "        .pipe(lambda df: pd.DataFrame(scaler.fit_transform(df), columns=df.columns))\n",
    "        .assign(y=df[output])\n",
    "    )\n",
    "    return normalized_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizovanje podataka RobustScaler-om\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "def normalize_data_robust(df,output):\n",
    "    scaler = RobustScaler()\n",
    "    normalized_df = (\n",
    "        df.drop(columns=[output])\n",
    "        .pipe(lambda df: pd.DataFrame(scaler.fit_transform(df), columns=df.columns))\n",
    "        .assign(y=df[output])\n",
    "    )\n",
    "    return normalized_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h3 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Tehnike standardizacije podataka</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizovanje podataka StandardScaler-om\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def standardize_data(df,output):\n",
    "    scaler = StandardScaler()\n",
    "    normalized_df = (\n",
    "        df.drop(columns=[output])\n",
    "        .pipe(lambda df: pd.DataFrame(scaler.fit_transform(df), columns=df.columns))\n",
    "        .assign(y=df[output])\n",
    "    )\n",
    "    return normalized_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h3 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Normalizovanje Bank dataseta</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primena MinMaxScalera na bank datasetu\n",
    "\n",
    "# Kombinovano enkodiranje\n",
    "normalized_bank_mix = normalize_data(bankDataset_MixEncoding,'y')\n",
    "# LabelEncoder\n",
    "normalized_bank_le = normalize_data(bankDataset_LabelEncoding,'y')\n",
    "# OneHotEncoder\n",
    "normalized_bank_oh = normalize_data(bankDataset_OneHotEncoding,'y')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h3 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Standardizovanje Bank dataseta</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primena StandardScalera na bank datasetu\n",
    "standardised_bank_mix = standardize_data(bankDataset_MixEncoding,'y')\n",
    "standardised_bank_le = standardize_data(bankDataset_LabelEncoding,'y')\n",
    "standardised_bank_oh = standardize_data(bankDataset_OneHotEncoding,'y')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h3 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Standardizovanje Airline dataseta</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_airline_mix = standardize_data(airlineDataset_MixEncoding,'satisfaction')\n",
    "standardized_airline_le = standardize_data(airlineDataset_LabelEncoding,'satisfaction')\n",
    "standardized_airline_oh = standardize_data(airlineDataset_OneHotEncoding,'satisfaction')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h3 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Normalizovanje Airline dataseta</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing Mixed Encoded dataset\n",
    "normalised_airline_mix = normalize_data(airlineDataset_MixEncoding,'satisfaction')\n",
    "# Normalizing Label Encoded dataset\n",
    "normalised_airline_le = normalize_data(airlineDataset_LabelEncoding,'satisfaction')\n",
    "# Normalizing OneHot Encoded dataset\n",
    "normalised_airline_oh = normalize_data(airlineDataset_OneHotEncoding,'satisfaction')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h3 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Testiranje SVM na Normalizovanom Bank datasetu</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM - Mixed Encoding - Airline normalized\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Extract the features and target variable\n",
    "X = normalised_bank_mix.drop(['y'],axis=1)\n",
    "y = normalised_bank_mix['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "# Train the SVM classifier on the training data\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)*100\n",
    "f1  = f1_score(y_test,y_pred)\n",
    "roc = roc_auc_score(y_test,y_pred)\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank Normalizovan\",\"MIX\",\"SVM\",f1,  roc,  '{:.2f}%'.format(accuracy)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM - Label Encoding - Airline normalized\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Extract the features and target variable\n",
    "X = normalised_bank_le.drop('y',axis=1)\n",
    "y = normalised_bank_le['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "# Train the SVM classifier on the training data\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)*100\n",
    "f1  = f1_score(y_test,y_pred)\n",
    "roc = roc_auc_score(y_test,y_pred)\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank Normalizovan\",\"Label Encoder\",\"SVM\",f1,  roc,  '{:.2f}%'.format(accuracy)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM - OneHot Encoding - Airline normalized\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Extract the features and target variable\n",
    "X_OH = normalised_bank_oh.drop('y',axis=1)\n",
    "y_OH = normalised_bank_oh['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_OH, y_OH, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "# Train the SVM classifier on the training data\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)*100\n",
    "f1  = f1_score(y_test,y_pred)\n",
    "roc = roc_auc_score(y_test,y_pred)\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank Normalizovan\",\"One Hot\",\"SVM\",f1,  roc,  '{:.2f}%'.format(accuracy)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h3 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Testiranje Naive Bayes na Normalizovanom Bank datasetu</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NaiveBayes on Mixed Encodings Bank dataset\n",
    "# Extract the features and target variable\n",
    "X_Bank_Mix_NB_N = normalised_bank_mix.drop('y',axis=1)\n",
    "y_Bank_Mix_NB_N = normalised_bank_mix['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_BM_NB, X_test_BM_NB, y_train_BM_NB, y_test_BM_NB = train_test_split(X_Bank_Mix_NB_N, y_Bank_Mix_NB_N, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Naive Bayes classifier\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Train the Naive Bayes classifier on the training data\n",
    "nb.fit(X_train_BM_NB, y_train_BM_NB)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred_BM_NB = nb.predict(X_test_BM_NB)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test_BM_NB, y_pred_BM_NB)*100\n",
    "roc = roc_auc_score(y_test_BM_NB, y_pred_BM_NB)\n",
    "f1 = f1_score(y_test_BM_NB, y_pred_BM_NB)\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank normalizovan\",\"MIX\",\"Naive Bayes\",f1,  roc,  '{:.2f}%'.format(accuracy)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NaiveBayes on Mixed Encodings Bank dataset\n",
    "# Extract the features and target variable\n",
    "X_Bank_Mix_NB_N = normalised_bank_le.drop('y',axis=1)\n",
    "y_Bank_Mix_NB_N = normalised_bank_le['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_BM_NB, X_test_BM_NB, y_train_BM_NB, y_test_BM_NB = train_test_split(X_Bank_Mix_NB_N, y_Bank_Mix_NB_N, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Naive Bayes classifier\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Train the Naive Bayes classifier on the training data\n",
    "nb.fit(X_train_BM_NB, y_train_BM_NB)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred_BM_NB = nb.predict(X_test_BM_NB)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test_BM_NB, y_pred_BM_NB)*100\n",
    "roc = roc_auc_score(y_test_BM_NB, y_pred_BM_NB)\n",
    "f1 = f1_score(y_test_BM_NB, y_pred_BM_NB)\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank normalizovan\",\"Label Encoder\",\"Naive Bayes\",f1,  roc,  '{:.2f}%'.format(accuracy)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NaiveBayes on Mixed Encodings Bank dataset\n",
    "# Extract the features and target variable\n",
    "X_Bank_Mix_NB_O = normalised_bank_oh.drop('y',axis=1)\n",
    "y_Bank_Mix_NB_O = normalised_bank_oh['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_BM_NB, X_test_BM_NB, y_train_BM_NB, y_test_BM_NB = train_test_split(X_Bank_Mix_NB_O, y_Bank_Mix_NB_O, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Naive Bayes classifier\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Train the Naive Bayes classifier on the training data\n",
    "nb.fit(X_train_BM_NB, y_train_BM_NB)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred_BM_NB = nb.predict(X_test_BM_NB)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test_BM_NB, y_pred_BM_NB)*100\n",
    "roc = roc_auc_score(y_test_BM_NB, y_pred_BM_NB)\n",
    "f1 = f1_score(y_test_BM_NB, y_pred_BM_NB)\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank normalizovan\",\"One Hot\",\"Naive Bayes\",f1,  roc,  '{:.2f}%'.format(accuracy)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h3 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Random Forest na normalizovanom Bank datasetu</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest - Mixed Encoding - Airline normalized\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Extract the features and target variable\n",
    "X_Airline_Mix_RF = normalised_bank_mix.drop('y',axis=1)\n",
    "y_Airline_Mix_RF = normalised_bank_mix['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_AM_RF, X_test_AM_RF, y_train_AM_RF, y_test_AM_RF = train_test_split(X_Airline_Mix_RF, y_Airline_Mix_RF, test_size=0.2, random_state=42)\n",
    "\n",
    "randomForest_Airline_Mix = RandomForestClassifier()\n",
    "randomForest_Airline_Mix.fit(X_train_AM_RF, y_train_AM_RF)\n",
    "\n",
    "pred = randomForest_Airline_Mix.predict(X_test_AM_RF)\n",
    "f1 = f1_score(y_test_AM_RF, randomForest_Airline_Mix.predict(X_test_AM_RF))\n",
    "auc = roc_auc_score(y_test_AM_RF, randomForest_Airline_Mix.predict(X_test_AM_RF))\n",
    "acc = accuracy_score(y_test_AM_RF, randomForest_Airline_Mix.predict(X_test_AM_RF))*100\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank Normalizovan\",\"MIX\",\"Random forest\",f1,  auc,  '{:.2f}%'.format(acc)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest - Label Encoding - Bank normalized\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Extract the features and target variable\n",
    "X_Airline_Mix_RF = normalised_bank_le.drop('y',axis=1)\n",
    "y_Airline_Mix_RF = normalised_bank_le['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_AM_RF, X_test_AM_RF, y_train_AM_RF, y_test_AM_RF = train_test_split(X_Airline_Mix_RF, y_Airline_Mix_RF, test_size=0.2, random_state=42)\n",
    "\n",
    "randomForest_Airline_Mix = RandomForestClassifier()\n",
    "randomForest_Airline_Mix.fit(X_train_AM_RF, y_train_AM_RF)\n",
    "\n",
    "pred = randomForest_Airline_Mix.predict(X_test_AM_RF)\n",
    "f1 = f1_score(y_test_AM_RF, randomForest_Airline_Mix.predict(X_test_AM_RF))\n",
    "auc = roc_auc_score(y_test_AM_RF, randomForest_Airline_Mix.predict(X_test_AM_RF))\n",
    "acc = accuracy_score(y_test_AM_RF, randomForest_Airline_Mix.predict(X_test_AM_RF))*100\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank Normalizovan\",\"Label Encoder\",\"Random forest\",f1,  auc,  '{:.2f}%'.format(acc)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest - OneHot Encoding - Airline normalized\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Extract the features and target variable\n",
    "X_Airline_Mix_RF = normalised_bank_oh.drop('y',axis=1)\n",
    "y_Airline_Mix_RF = normalised_bank_oh['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_AM_RF, X_test_AM_RF, y_train_AM_RF, y_test_AM_RF = train_test_split(X_Airline_Mix_RF, y_Airline_Mix_RF, test_size=0.2, random_state=42)\n",
    "\n",
    "randomForest_Airline_Mix = RandomForestClassifier()\n",
    "randomForest_Airline_Mix.fit(X_train_AM_RF, y_train_AM_RF)\n",
    "\n",
    "pred = randomForest_Airline_Mix.predict(X_test_AM_RF)\n",
    "f1 = f1_score(y_test_AM_RF, randomForest_Airline_Mix.predict(X_test_AM_RF))\n",
    "auc = roc_auc_score(y_test_AM_RF, randomForest_Airline_Mix.predict(X_test_AM_RF))\n",
    "acc = accuracy_score(y_test_AM_RF, randomForest_Airline_Mix.predict(X_test_AM_RF))*100\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank Normalizovan\",\"One Hot\",\"Random forest\",f1,  auc,  '{:.2f}%'.format(acc)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h3 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "ANN nad Bank normalizovanim skupom podataka</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN - Mixed Encoding - Airline normalized\n",
    "\n",
    "X_Bank_AM_ANN_N = normalised_bank_mix.drop(['y'],axis=1)\n",
    "y_Bank_AM_ANN_N = normalised_bank_mix['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_AM_ANN, X_test_AM_ANN, y_train_AM_ANN, y_test_AM_ANN = train_test_split(X_Bank_AM_ANN_N, y_Bank_AM_ANN_N, test_size=0.2, random_state=42)\n",
    "\n",
    "model_AM_ANN = Sequential()\n",
    "model_AM_ANN.add(Dense(512, activation='relu', input_shape=(X_train_AM_ANN.shape[1],)))\n",
    "model_AM_ANN.add(Dropout(0.3))\n",
    "model_AM_ANN.add(Dense(256, activation='relu'))\n",
    "model_AM_ANN.add(Dense(256, activation='relu'))\n",
    "model_AM_ANN.add(Dropout(0.3))\n",
    "model_AM_ANN.add(Dense(128, activation='relu'))\n",
    "model_AM_ANN.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model_AM_ANN.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', \n",
    "                   mode='max', \n",
    "                   patience=10,  #Stop the model training if the validation accuracy doesnt increase in 10 Epochs\n",
    "                   restore_best_weights=True)\n",
    "\n",
    "NN_Classifier_AM_N = model_AM_ANN.fit(X_train_AM_ANN,y_train_AM_ANN,\n",
    "                          batch_size=1000,\n",
    "                          epochs=1000,\n",
    "                          callbacks =[es],\n",
    "                          verbose=2,\n",
    "                          validation_data=(X_test_AM_ANN, y_test_AM_ANN))\n",
    "                          \n",
    "\n",
    "y_pred=(model_AM_ANN.predict(X_test_AM_ANN) > 0.5).astype(int)\n",
    "acc = accuracy_score(y_test_AM_ANN, y_pred)*100\n",
    "#print('Accuracy Score: ', '{:.2f}%'.format(acc))\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank normalizovan\",\"MIX\",\"ANN\",0,  0, '{:.2f}%'.format(acc)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN - Label Encoding - Airline normalized\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "X_Bank_BM_ANN = normalised_bank_le.drop('y',axis=1)\n",
    "y_Bank_BM_ANN = normalised_bank_le['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_AM_ANN, X_test_AM_ANN, y_train_AM_ANN, y_test_AM_ANN = train_test_split(X_Bank_BM_ANN, y_Bank_BM_ANN, test_size=0.2, random_state=42)\n",
    "\n",
    "model_AM_ANN = Sequential()\n",
    "model_AM_ANN.add(Dense(512, activation='relu', input_shape=(X_train_AM_ANN.shape[1],)))\n",
    "model_AM_ANN.add(Dropout(0.3))\n",
    "model_AM_ANN.add(BatchNormalization())\n",
    "model_AM_ANN.add(Dense(256, activation='relu'))\n",
    "model_AM_ANN.add(Dropout(0.3))\n",
    "model_AM_ANN.add(BatchNormalization())\n",
    "model_AM_ANN.add(Dense(128, activation='relu'))\n",
    "model_AM_ANN.add(Dropout(0.3))\n",
    "model_AM_ANN.add(BatchNormalization())\n",
    "model_AM_ANN.add(Dense(64, activation='relu'))\n",
    "model_AM_ANN.add(Dropout(0.3))\n",
    "model_AM_ANN.add(BatchNormalization())\n",
    "model_AM_ANN.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model_AM_ANN.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', \n",
    "                   mode='max', \n",
    "                   patience=10,  #Stop the model training if the validation accuracy doesnt increase in 10 Epochs\n",
    "                   restore_best_weights=True)\n",
    "\n",
    "NN_Classifier_AM = model_AM_ANN.fit(X_train_AM_ANN,y_train_AM_ANN,\n",
    "                          batch_size=1000,\n",
    "                          epochs=1000,\n",
    "                          callbacks =[es],\n",
    "                          verbose=2,\n",
    "                          validation_data=(X_test_AM_ANN, y_test_AM_ANN))\n",
    "                          \n",
    "\n",
    "y_pred=(model_AM_ANN.predict(X_test_AM_ANN) > 0.5).astype(int)\n",
    "acc = accuracy_score(y_test_AM_ANN, y_pred)*100\n",
    "#print('Accuracy Score: ', '{:.2f}%'.format(acc))\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank normalizovan\",\"Label Encoder\",\"ANN\",0,  0, '{:.2f}%'.format(acc)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN - OneHot Encoding - Airline normalized\n",
    "\n",
    "X_Airline_BM_ANN = normalised_airline_oh\n",
    "y_Airline_BM_ANN = airlineDataset_OneHotEncoding['satisfaction']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_AM_ANN, X_test_AM_ANN, y_train_AM_ANN, y_test_AM_ANN = train_test_split(X_Airline_BM_ANN, y_Airline_BM_ANN, test_size=0.2, random_state=42)\n",
    "\n",
    "model_AM_ANN = Sequential()\n",
    "model_AM_ANN.add(Dense(512, activation='relu', input_shape=(X_train_AM_ANN.shape[1],)))\n",
    "model_AM_ANN.add(Dropout(0.3))\n",
    "model_AM_ANN.add(Dense(256, activation='relu'))\n",
    "model_AM_ANN.add(Dense(256, activation='relu'))\n",
    "model_AM_ANN.add(Dropout(0.3))\n",
    "model_AM_ANN.add(Dense(128, activation='relu'))\n",
    "model_AM_ANN.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model_AM_ANN.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', \n",
    "                   mode='max', \n",
    "                   patience=10,  #Stop the model training if the validation accuracy doesnt increase in 10 Epochs\n",
    "                   restore_best_weights=True)\n",
    "\n",
    "NN_Classifier_AM = model_AM_ANN.fit(X_train_AM_ANN,y_train_AM_ANN,\n",
    "                          batch_size=1000,\n",
    "                          epochs=1000,\n",
    "                          callbacks =[es],\n",
    "                          verbose=2,\n",
    "                          validation_data=(X_test_AM_ANN, y_test_AM_ANN))\n",
    "                          \n",
    "\n",
    "y_pred=(model_AM_ANN.predict(X_test_AM_ANN) > 0.5).astype(int)\n",
    "acc = accuracy_score(y_test_AM_ANN, y_pred)*100\n",
    "#print('Accuracy Score: ', '{:.2f}%'.format(acc))\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Airline normalizovan\",\"One Hot\",\"ANN\",0,  0, '{:.2f}%'.format(acc)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h3 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "ANN nad Bank standardizovanim skupom podataka</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_airline_mix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN - Mixed Encoding - Airline standardized\n",
    "\n",
    "X_Airline_BM_ANN_S = standardized_airline_mix.drop('satisfaction',axis=1)\n",
    "y_Airline_BM_ANN_S = airlineDataset_MixEncoding['satisfaction']\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_AM_ANN, X_test_AM_ANN, y_train_AM_ANN, y_test_AM_ANN = train_test_split(X_Airline_BM_ANN_S, y_Airline_BM_ANN_S, test_size=0.2)\n",
    "\n",
    "model_AM_ANN_M = Sequential()\n",
    "model_AM_ANN_M.add(Dense(512, activation='relu', input_shape=(X_train_AM_ANN.shape[1],)))\n",
    "model_AM_ANN_M.add(Dropout(0.3))\n",
    "model_AM_ANN_M.add(Dense(256, activation='relu'))\n",
    "model_AM_ANN_M.add(Dense(256, activation='relu'))\n",
    "model_AM_ANN_M.add(Dropout(0.3))\n",
    "model_AM_ANN_M.add(Dense(128, activation='relu'))\n",
    "model_AM_ANN_M.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model_AM_ANN_M.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', \n",
    "                   mode='max', \n",
    "                   patience=10,  #Stop the model training if the validation accuracy doesnt increase in 10 Epochs\n",
    "                   restore_best_weights=True)\n",
    "\n",
    "NN_Classifier_AM_S = model_AM_ANN_M.fit(X_train_AM_ANN,y_train_AM_ANN,\n",
    "                          batch_size=1000,\n",
    "                          epochs=1000,\n",
    "                          callbacks =[es],\n",
    "                          verbose=2,\n",
    "                          validation_data=(X_test_AM_ANN, y_test_AM_ANN))\n",
    "\n",
    "y_pred=(model_AM_ANN_M.predict(X_test_AM_ANN) > 0.5).astype(int)\n",
    "acc = accuracy_score(y_test_AM_ANN, y_pred)*100\n",
    "#print('Accuracy Score: ', '{:.2f}%'.format(acc))\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Airline Standaradizovan\",\"MIX\",\"ANN\",0,  0, '{:.2f}%'.format(acc)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN - Label Encoding - Airline standardized\n",
    "\n",
    "X_Airline_BM_ANN_S_L = standardized_airline_le.drop('satisfaction',axis=1)\n",
    "y_Airline_BM_ANN_S_L = airlineDataset_LabelEncoding['satisfaction']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_AM_ANN, X_test_AM_ANN, y_train_AM_ANN, y_test_AM_ANN = train_test_split(X_Airline_BM_ANN_S_L, y_Airline_BM_ANN_S_L, test_size=0.2, random_state=42)\n",
    "\n",
    "model_AM_ANN = Sequential()\n",
    "model_AM_ANN.add(Dense(512, activation='relu', input_shape=(X_train_AM_ANN.shape[1],)))\n",
    "model_AM_ANN.add(Dropout(0.3))\n",
    "model_AM_ANN.add(Dense(256, activation='relu'))\n",
    "model_AM_ANN.add(Dense(256, activation='relu'))\n",
    "model_AM_ANN.add(Dropout(0.3))\n",
    "model_AM_ANN.add(Dense(128, activation='relu'))\n",
    "model_AM_ANN.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model_AM_ANN.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', \n",
    "                   mode='max', \n",
    "                   patience=10,  #Stop the model training if the validation accuracy doesnt increase in 10 Epochs\n",
    "                   restore_best_weights=True)\n",
    "\n",
    "NN_Classifier_AM_SL = model_AM_ANN.fit(X_train_AM_ANN,y_train_AM_ANN,\n",
    "                          batch_size=1000,\n",
    "                          epochs=1000,\n",
    "                          callbacks =[es],\n",
    "                          verbose=2,\n",
    "                          validation_data=(X_test_AM_ANN, y_test_AM_ANN))\n",
    "                          \n",
    "\n",
    "y_pred=(model_AM_ANN.predict(X_test_AM_ANN) > 0.5).astype(int)\n",
    "acc = accuracy_score(y_test_AM_ANN, y_pred)*100\n",
    "#print('Accuracy Score: ', '{:.2f}%'.format(acc))\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Airline Standardizovan\",\"Label Encoder\",\"ANN\",0,  0, '{:.2f}%'.format(acc)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN - OneHot Encoding - Airline standardized\n",
    "\n",
    "X_Airline_BM_ANN_S_OH = standardized_airline_oh.drop('satisfaction',axis=1)\n",
    "y_Airline_BM_ANN_S_OH = airlineDataset_OneHotEncoding['satisfaction']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_AM_ANN, X_test_AM_ANN, y_train_AM_ANN, y_test_AM_ANN = train_test_split(X_Airline_BM_ANN_S_OH, y_Airline_BM_ANN_S_OH, test_size=0.2, random_state=42)\n",
    "\n",
    "model_AM_ANN_OH = Sequential()\n",
    "model_AM_ANN_OH.add(Dense(512, activation='relu', input_shape=(X_train_AM_ANN.shape[1],)))\n",
    "model_AM_ANN_OH.add(Dropout(0.3))\n",
    "model_AM_ANN_OH.add(Dense(256, activation='relu'))\n",
    "model_AM_ANN_OH.add(Dense(256, activation='relu'))\n",
    "model_AM_ANN_OH.add(Dropout(0.3))\n",
    "model_AM_ANN_OH.add(Dense(128, activation='relu'))\n",
    "model_AM_ANN_OH.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model_AM_ANN_OH.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', \n",
    "                   mode='max', \n",
    "                   patience=10,  #Stop the model training if the validation accuracy doesnt increase in 10 Epochs\n",
    "                   restore_best_weights=True)\n",
    "\n",
    "NN_Classifier_AM_SOH = model_AM_ANN_OH.fit(X_train_AM_ANN,y_train_AM_ANN,\n",
    "                          batch_size=1000,\n",
    "                          epochs=1000,\n",
    "                          callbacks =[es],\n",
    "                          verbose=2,\n",
    "                          validation_data=(X_test_AM_ANN, y_test_AM_ANN))\n",
    "                          \n",
    "\n",
    "y_pred=(model_AM_ANN_OH.predict(X_test_AM_ANN) > 0.5).astype(int)\n",
    "acc = accuracy_score(y_test_AM_ANN, y_pred)*100\n",
    "#print('Accuracy Score: ', '{:.2f}%'.format(acc))\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Airline Standardizovan\",\"One Hot\",\"ANN\",0,  0, '{:.2f}%'.format(acc)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h3 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "ANN nad Bank standardizovanim skupom podataka</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN - Mixed Encoding - Bank dataset\n",
    "X_Bank_BM_ANN = standardised_bank_mix.drop(['y'], axis=1)\n",
    "y_Bank_BM_ANN = bankDataset_MixEncoding['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_BM_ANN, X_test_BM_ANN, y_train_BM_ANN, y_test_BM_ANN = train_test_split(X_Bank_BM_ANN, y_Bank_BM_ANN, test_size=0.2, random_state=42)\n",
    "\n",
    "model_BM_ANN = Sequential()\n",
    "model_BM_ANN.add(Dense(512, activation='relu', input_shape=(X_train_BM_ANN.shape[1],)))\n",
    "model_BM_ANN.add(Dropout(0.3))\n",
    "model_BM_ANN.add(Dense(256, activation='relu'))\n",
    "model_BM_ANN.add(Dense(256, activation='relu'))\n",
    "model_BM_ANN.add(Dropout(0.3))\n",
    "model_BM_ANN.add(Dense(128, activation='relu'))\n",
    "model_BM_ANN.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model_BM_ANN.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', \n",
    "                   mode='max', \n",
    "                   patience=10,  #Stop the model training if the validation accuracy doesnt increase in 10 Epochs\n",
    "                   restore_best_weights=True)\n",
    "\n",
    "NN_Classifier_BM = model_BM_ANN.fit(X_train_BM_ANN,y_train_BM_ANN,\n",
    "                          batch_size=1000,\n",
    "                          epochs=1000,\n",
    "                          callbacks =[es],\n",
    "                          verbose=2,\n",
    "                          validation_data=(X_test_BM_ANN, y_test_BM_ANN))\n",
    "                          \n",
    "\n",
    "y_pred=(model_BM_ANN.predict(X_test_BM_ANN) > 0.5).astype(int)\n",
    "acc = accuracy_score(y_test_BM_ANN, y_pred)*100\n",
    "#print('Accuracy Score: ', '{:.2f}%'.format(acc))\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank standardizovan\",\"MIX\",\"ANN\",0,  0, '{:.2f}%'.format(acc)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN - Label Encoding -  Bank dataset\n",
    "X_Bank_BLE_ANN = standardised_bank_le.drop(['y'], axis=1)\n",
    "y_Bank_BLE_ANN = bankDataset_LabelEncoding['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_BLE_ANN, X_test_BLE_ANN, y_train_BLE_ANN, y_test_BLE_ANN = train_test_split(X_Bank_BLE_ANN, y_Bank_BLE_ANN, test_size=0.2, random_state=42)\n",
    "\n",
    "model_BLE_ANN = Sequential()\n",
    "model_BLE_ANN.add(Dense(512, activation='relu', input_shape=(X_train_BLE_ANN.shape[1],)))\n",
    "model_BLE_ANN.add(Dropout(0.3))\n",
    "model_BLE_ANN.add(Dense(256, activation='relu'))\n",
    "model_BLE_ANN.add(Dense(256, activation='relu'))\n",
    "model_BLE_ANN.add(Dropout(0.3))\n",
    "model_BLE_ANN.add(Dense(128, activation='relu'))\n",
    "model_BLE_ANN.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model_BLE_ANN.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', \n",
    "                   mode='max', \n",
    "                   patience=10,  #Stop the model training if the validation accuracy doesnt increase in 10 Epochs\n",
    "                   restore_best_weights=True)\n",
    "\n",
    "NN_Classifier_BLE = model_BLE_ANN.fit(X_train_BLE_ANN, y_train_BLE_ANN,\n",
    "                          batch_size=1000,\n",
    "                          epochs=1000,\n",
    "                          callbacks =[es],\n",
    "                          verbose=2,\n",
    "                          validation_data=(X_test_BLE_ANN, y_test_BLE_ANN))\n",
    "\n",
    "\n",
    "y_pred=(model_BLE_ANN.predict(X_test_BLE_ANN) > 0.5).astype(int)\n",
    "acc = accuracy_score(y_test_BLE_ANN, y_pred)*100\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank standardizovan\",\"Label Encoder\",\"ANN\",0,  0,  '{:.2f}%'.format(acc)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN - One Hot Encoding -  Bank dataset\n",
    "X_Bank_BOH_ANN = standardised_bank_oh.drop(['y'], axis=1)\n",
    "y_Bank_BOH_ANN = bankDataset_OneHotEncoding['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_BOH_ANN, X_test_BOH_ANN, y_train_BOH_ANN, y_test_BOH_ANN = train_test_split(X_Bank_BOH_ANN, y_Bank_BOH_ANN, test_size=0.2, random_state=42)\n",
    "\n",
    "model_BOH_ANN = Sequential()\n",
    "model_BOH_ANN.add(Dense(512, activation='relu', input_shape=(X_train_BOH_ANN.shape[1],)))\n",
    "model_BOH_ANN.add(Dropout(0.3))\n",
    "model_BOH_ANN.add(Dense(256, activation='relu'))\n",
    "model_BOH_ANN.add(Dense(256, activation='relu'))\n",
    "model_BOH_ANN.add(Dropout(0.3))\n",
    "model_BOH_ANN.add(Dense(128, activation='relu'))\n",
    "model_BOH_ANN.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model_BOH_ANN.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', \n",
    "                   mode='max', \n",
    "                   patience=10,  #Stop the model training if the validation accuracy doesnt increase in 10 Epochs\n",
    "                   restore_best_weights=True)\n",
    "\n",
    "NN_Classifier_BOH = model_BOH_ANN.fit(X_train_BOH_ANN, y_train_BOH_ANN,\n",
    "                          batch_size=1000,\n",
    "                          epochs=1000,\n",
    "                          callbacks =[es],\n",
    "                          verbose=2,\n",
    "                          validation_data=(X_test_BOH_ANN, y_test_BOH_ANN))\n",
    "\n",
    "\n",
    "y_pred=(model_BOH_ANN.predict(X_test_BOH_ANN) > 0.5).astype(int)\n",
    "acc = accuracy_score(y_test_BOH_ANN, y_pred)*100\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank standardizovan\",\"One Hot\",\"ANN\",0,  0,  '{:.2f}%'.format(acc)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h3 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "ANN nad Bank normalizovanim skupom podataka</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN - Mixed Encoding - Bank dataset\n",
    "X_Bank_BM_ANN = normalised_bank_mix.drop('y',axis=1)\n",
    "y_Bank_BM_ANN = bankDataset_MixEncoding['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_BM_ANN, X_test_BM_ANN, y_train_BM_ANN, y_test_BM_ANN = train_test_split(X_Bank_BM_ANN, y_Bank_BM_ANN, test_size=0.2, random_state=42)\n",
    "\n",
    "model_BM_ANN = Sequential()\n",
    "model_BM_ANN.add(Dense(512, activation='relu', input_shape=(X_train_BM_ANN.shape[1],)))\n",
    "model_BM_ANN.add(Dropout(0.3))\n",
    "model_BM_ANN.add(Dense(256, activation='relu'))\n",
    "model_BM_ANN.add(Dense(256, activation='relu'))\n",
    "model_BM_ANN.add(Dropout(0.3))\n",
    "model_BM_ANN.add(Dense(128, activation='relu'))\n",
    "model_BM_ANN.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model_BM_ANN.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', \n",
    "                   mode='max', \n",
    "                   patience=10,  #Stop the model training if the validation accuracy doesnt increase in 10 Epochs\n",
    "                   restore_best_weights=True)\n",
    "\n",
    "NN_Classifier_BM = model_BM_ANN.fit(X_train_BM_ANN,y_train_BM_ANN,\n",
    "                          batch_size=1000,\n",
    "                          epochs=1000,\n",
    "                          callbacks =[es],\n",
    "                          verbose=2,\n",
    "                          validation_data=(X_test_BM_ANN, y_test_BM_ANN))\n",
    "                          \n",
    "\n",
    "y_pred=(model_BM_ANN.predict(X_test_BM_ANN) > 0.5).astype(int)\n",
    "acc = accuracy_score(y_test_BM_ANN, y_pred)*100\n",
    "#print('Accuracy Score: ', '{:.2f}%'.format(acc))\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank normalizovan\",\"MIX\",\"ANN\",0,  0, '{:.2f}%'.format(acc)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN - Label Encoding -  Bank dataset\n",
    "X_Bank_BLE_ANN = normalised_bank_le.drop('y',axis=1)\n",
    "y_Bank_BLE_ANN = bankDataset_LabelEncoding['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_BLE_ANN, X_test_BLE_ANN, y_train_BLE_ANN, y_test_BLE_ANN = train_test_split(X_Bank_BLE_ANN, y_Bank_BLE_ANN, test_size=0.2, random_state=42)\n",
    "\n",
    "model_BLE_ANN = Sequential()\n",
    "model_BLE_ANN.add(Dense(512, activation='relu', input_shape=(X_train_BLE_ANN.shape[1],)))\n",
    "model_BLE_ANN.add(Dropout(0.3))\n",
    "model_BLE_ANN.add(Dense(256, activation='relu'))\n",
    "model_BLE_ANN.add(Dense(256, activation='relu'))\n",
    "model_BLE_ANN.add(Dropout(0.3))\n",
    "model_BLE_ANN.add(Dense(128, activation='relu'))\n",
    "model_BLE_ANN.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model_BLE_ANN.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', \n",
    "                   mode='max', \n",
    "                   patience=10,  #Stop the model training if the validation accuracy doesnt increase in 10 Epochs\n",
    "                   restore_best_weights=True)\n",
    "\n",
    "NN_Classifier_BLE = model_BLE_ANN.fit(X_train_BLE_ANN, y_train_BLE_ANN,\n",
    "                          batch_size=1000,\n",
    "                          epochs=1000,\n",
    "                          callbacks =[es],\n",
    "                          verbose=2,\n",
    "                          validation_data=(X_test_BLE_ANN, y_test_BLE_ANN))\n",
    "\n",
    "\n",
    "y_pred=(model_BLE_ANN.predict(X_test_BLE_ANN) > 0.5).astype(int)\n",
    "acc = accuracy_score(y_test_BLE_ANN, y_pred)*100\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank normalizovan\",\"Label Encoder\",\"ANN\",0,  0,  '{:.2f}%'.format(acc)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN - One Hot Encoding -  Bank dataset\n",
    "X_Bank_BOH_ANN = normalised_bank_oh.drop('y',axis=1)\n",
    "y_Bank_BOH_ANN = bankDataset_OneHotEncoding['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_BOH_ANN, X_test_BOH_ANN, y_train_BOH_ANN, y_test_BOH_ANN = train_test_split(X_Bank_BOH_ANN, y_Bank_BOH_ANN, test_size=0.2, random_state=42)\n",
    "\n",
    "model_BOH_ANN = Sequential()\n",
    "model_BOH_ANN.add(Dense(512, activation='relu', input_shape=(X_train_BOH_ANN.shape[1],)))\n",
    "model_BOH_ANN.add(Dropout(0.3))\n",
    "model_BOH_ANN.add(Dense(256, activation='relu'))\n",
    "model_BOH_ANN.add(Dense(256, activation='relu'))\n",
    "model_BOH_ANN.add(Dropout(0.3))\n",
    "model_BOH_ANN.add(Dense(128, activation='relu'))\n",
    "model_BOH_ANN.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model_BOH_ANN.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', \n",
    "                   mode='max', \n",
    "                   patience=10,  #Stop the model training if the validation accuracy doesnt increase in 10 Epochs\n",
    "                   restore_best_weights=True)\n",
    "\n",
    "NN_Classifier_BOH = model_BOH_ANN.fit(X_train_BOH_ANN, y_train_BOH_ANN,\n",
    "                          batch_size=1000,\n",
    "                          epochs=1000,\n",
    "                          callbacks =[es],\n",
    "                          verbose=2,\n",
    "                          validation_data=(X_test_BOH_ANN, y_test_BOH_ANN))\n",
    "\n",
    "\n",
    "y_pred=(model_BOH_ANN.predict(X_test_BOH_ANN) > 0.5).astype(int)\n",
    "acc = accuracy_score(y_test_BOH_ANN, y_pred)*100\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank normalizovan\",\"One Hot\",\"ANN\",0,  0,  '{:.2f}%'.format(acc)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poredjenje Modela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = testingDataframe[testingDataframe['Algoritam']== 'ANN']\n",
    "testing = testing.sort_values(by=['Accuracy'],ascending=False)\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = testingDataframe[testingDataframe['Algoritam']== 'SVM']\n",
    "testing = testing.sort_values(by=['Accuracy'],ascending=False)\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = testingDataframe[testingDataframe['Algoritam']== 'Random forest']\n",
    "testing = testing.sort_values(by=['Accuracy'],ascending=False)\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = testingDataframe[testingDataframe['Algoritam']== 'Naive Bayes']\n",
    "testing = testing.sort_values(by=['Accuracy'],ascending=False)\n",
    "testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h2 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Rukovanje ekstremnih vrednosti</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kopiranje dataseta za handlovanje outliera\n",
    "\n",
    "normalised_bank_mix_ro = normalised_bank_mix.copy()\n",
    "normalised_bank_mix_io = normalised_bank_mix.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcija za crtanje Box Plota\n",
    "def BoxPlot(df):\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    sns.boxplot(df)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcija za crtanje Histogram plota\n",
    "def HistPlot(df):\n",
    "    plt.figure(figsize=(5,2))\n",
    "    plt.hist(df)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crtanje Box Plot-ova\n",
    "for col in checkOutliers_bank:\n",
    "    BoxPlot(normalised_bank_mix[col])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crtanje Hist Plot-ova\n",
    "for col in checkOutliers_bank:\n",
    "    HistPlot(normalised_bank_mix[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_Bank = ['age','duration','campaign', 'cons.conf.idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funckija za brisanje outliera\n",
    "def drop_outliers(df,fieldname):\n",
    "    iqr = 1.5*(np.percentile(df[fieldname],75) - np.percentile(df[fieldname],25))\n",
    "    df.drop(df[df[fieldname] > (iqr + np.percentile(df[fieldname],75))].index,inplace=True)\n",
    "    df.drop(df[df[fieldname] < ( np.percentile(df[fieldname],25) - iqr)].index,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funckija za imputaciju outliera\n",
    "def ImputateOutlier(df):\n",
    "    q1 = df.quantile(0.25)\n",
    "    q3 = df.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    LT = q1 - 1.5*iqr\n",
    "    UT = q3 + 1.5*iqr\n",
    "    df = df.clip(lower=LT, upper=UT)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputacija outliera\n",
    "for col in outliers_Bank:\n",
    "    ImputateOutlier(normalised_bank_mix_io[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brisanje outliera\n",
    "for col in outliers_Bank:\n",
    "    drop_outliers(normalised_bank_mix_ro,col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crtanje Box Plot-ova nakon imputacije\n",
    "for col in checkOutliers_bank:\n",
    "    BoxPlot(normalised_bank_mix_io[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crtanje Box Plot-ova nakon brisanja\n",
    "for col in checkOutliers_bank:\n",
    "    BoxPlot(normalised_bank_mix_ro[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN - Mixed Encoding - Bank dataset - Without outliers\n",
    "X_Bank_BM_ANN = normalised_bank_mix_ro.drop('y', axis=1)\n",
    "y_Bank_BM_ANN = normalised_bank_mix_ro['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_BM_ANN, X_test_BM_ANN, y_train_BM_ANN, y_test_BM_ANN = train_test_split(X_Bank_BM_ANN, y_Bank_BM_ANN, test_size=0.2, random_state=42)\n",
    "\n",
    "model_BM_ANN = Sequential()\n",
    "model_BM_ANN.add(Dense(512, activation='relu', input_shape=(X_train_BM_ANN.shape[1],)))\n",
    "model_BM_ANN.add(Dropout(0.3))\n",
    "model_BM_ANN.add(Dense(256, activation='relu'))\n",
    "model_BM_ANN.add(Dense(256, activation='relu'))\n",
    "model_BM_ANN.add(Dropout(0.3))\n",
    "model_BM_ANN.add(Dense(128, activation='relu'))\n",
    "model_BM_ANN.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model_BM_ANN.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', \n",
    "                   mode='max', \n",
    "                   patience=10,  #Stop the model training if the validation accuracy doesnt increase in 10 Epochs\n",
    "                   restore_best_weights=True)\n",
    "\n",
    "NN_Classifier_BM = model_BM_ANN.fit(X_train_BM_ANN,y_train_BM_ANN,\n",
    "                          batch_size=1000,\n",
    "                          epochs=1000,\n",
    "                          callbacks =[es],\n",
    "                          verbose=2,\n",
    "                          validation_data=(X_test_BM_ANN, y_test_BM_ANN))\n",
    "                          \n",
    "\n",
    "y_pred=(model_BM_ANN.predict(X_test_BM_ANN) > 0.5).astype(int)\n",
    "acc = accuracy_score(y_test_BM_ANN, y_pred)*100\n",
    "#print('Accuracy Score: ', '{:.2f}%'.format(acc))\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank normalizovan w/ Outliers\",\"MIX\",\"ANN\",0,  0, '{:.2f}%'.format(acc)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN - Mixed Encoding - Bank dataset - Without outliers\n",
    "X_Bank_BM_ANN = normalised_bank_mix_io.drop('y', axis=1)\n",
    "y_Bank_BM_ANN = normalised_bank_mix_io['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_BM_ANN, X_test_BM_ANN, y_train_BM_ANN, y_test_BM_ANN = train_test_split(X_Bank_BM_ANN, y_Bank_BM_ANN, test_size=0.2, random_state=42)\n",
    "\n",
    "model_BM_ANN = Sequential()\n",
    "model_BM_ANN.add(Dense(512, activation='relu', input_shape=(X_train_BM_ANN.shape[1],)))\n",
    "model_BM_ANN.add(Dropout(0.3))\n",
    "model_BM_ANN.add(Dense(256, activation='relu'))\n",
    "model_BM_ANN.add(Dense(256, activation='relu'))\n",
    "model_BM_ANN.add(Dropout(0.3))\n",
    "model_BM_ANN.add(Dense(128, activation='relu'))\n",
    "model_BM_ANN.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model_BM_ANN.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', \n",
    "                   mode='max', \n",
    "                   patience=10,  #Stop the model training if the validation accuracy doesnt increase in 10 Epochs\n",
    "                   restore_best_weights=True)\n",
    "\n",
    "NN_Classifier_BM = model_BM_ANN.fit(X_train_BM_ANN,y_train_BM_ANN,\n",
    "                          batch_size=1000,\n",
    "                          epochs=1000,\n",
    "                          callbacks =[es],\n",
    "                          verbose=2,\n",
    "                          validation_data=(X_test_BM_ANN, y_test_BM_ANN))\n",
    "                          \n",
    "\n",
    "y_pred=(model_BM_ANN.predict(X_test_BM_ANN) > 0.5).astype(int)\n",
    "acc = accuracy_score(y_test_BM_ANN, y_pred)*100\n",
    "#print('Accuracy Score: ', '{:.2f}%'.format(acc))\n",
    "\n",
    "testingDataframe.loc[-1] = [\"Bank normalizovan Imputated Outliers\",\"MIX\",\"ANN\",0,  0, '{:.2f}%'.format(acc)]\n",
    "testingDataframe.index = testingDataframe.index + 1\n",
    "testing = testingDataframe.sort_index()\n",
    "testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h2 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Diskretizacija podataka</h2>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h3 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Airline dataset</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlineDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlineDataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns for discretization\n",
    "    # Age, Flight Distance, Grades, Delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlineDataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_discretization_columns = ['Age','Flight Distance','Departure Delay in Minutes','Arrival Delay in Minutes']\n",
    "airline_rating_columns = airlineDataset.columns[7:21]\n",
    "airline_rating_columns_names = list(airline_rating_columns)\n",
    "airline_rating_columns_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# Select the variable to discretize\n",
    "age = airlineDataset[['Age']]\n",
    "\n",
    "# Create a KBinsDiscretizer object with 5 bins\n",
    "kbins = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n",
    "\n",
    "# Fit and transform the age variable\n",
    "age_binned = kbins.fit_transform(age)\n",
    "\n",
    "# Print the resulting binned values\n",
    "airlineDataset['Age'] = age_binned\n",
    "\n",
    "airlineDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlineDataset['Arrival Delay in Minutes'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace values of 0 with 'On time' and values of 1 or more with 'Delayed'\n",
    "airlineDataset['Arrival Delay in Minutes'] = airlineDataset['Arrival Delay in Minutes'].replace(0, 'On time').replace(range(1, int(airlineDataset['Arrival Delay in Minutes'].max()) + 1), 'Delayed')\n",
    "\n",
    "# Print the resulting value counts\n",
    "print(airlineDataset['Arrival Delay in Minutes'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace values of 0 with 'On time' and values of 1 or more with 'Delayed'\n",
    "airlineDataset['Departure Delay in Minutes'] = airlineDataset['Departure Delay in Minutes'].replace(0, 'On time').replace(range(1, airlineDataset['Departure Delay in Minutes'].max() + 1), 'Delayed')\n",
    "\n",
    "# Print the resulting value counts\n",
    "print(airlineDataset['Departure Delay in Minutes'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labele=['Unsatisfied','Neutral','Satisfied']\n",
    "binovi = [0,1,3,5]\n",
    "for label in airline_rating_columns_names:\n",
    "    airlineDataset[label] = pd.cut(airlineDataset[label],binovi,labele)\n",
    "airlineDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in airline_rating_columns_names:\n",
    "    airlineDataset[label] = pd.cut(airlineDataset[label],bins=3,labels=['Unsatisfied','Neutral','Satisfied'])\n",
    "airlineDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Odabir atributa\n",
    "flight_distance = airlineDataset[['Flight Distance']]\n",
    "\n",
    "# Kreiranje KBinsDiscretizer objekta s 3 kategorije\n",
    "kbins = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='kmeans')\n",
    "\n",
    "# Primjena diskretizacije na atribut Flight Distance\n",
    "flight_distance_binned = kbins.fit_transform(flight_distance)\n",
    "\n",
    "# Dodavanje kategorija u dataset\n",
    "airlineDataset['Flight Distance'] = flight_distance_binned\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in airlineDataset.columns:\n",
    "    print(airlineDataset[column].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px;\n",
    "            background-color:#D6EAF8;\n",
    "            letter-spacing:0.5px; text-align: center;\">\n",
    "            \n",
    "<h2 style=\"padding: 5px 0px; color:#2980B9; font-weight: bold; font-family: Arial, sans-serif;\">\n",
    "Promena raspodele podataka</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehiclesDataset = vehiclesDataset.drop(['angle', 'pos', 'lane', 'slope', 'signals', 'posLat'], axis=1)\\\n",
    "       .rename(columns={'x': 'longitude', 'y': 'latitude'})\\\n",
    "       .assign(speed_kmh=lambda x: round(x['speed'] * 3.6, 2))\\\n",
    "       .drop(['speed'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehiclesDataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehiclesDataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehiclesDataset = vehiclesDataset[['id','timestep','type','latitude','longitude','speed_kmh','acceleration','distance','odometer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehiclesDataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehiclesDataset['type'] = vehiclesDataset['type'].replace({'bus_bus':'bus', 'veh_passenger':'car'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehiclesDataset['timestep'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehiclesDataset['time_of_day'] = vehiclesDataset['timestep'].apply(lambda x: 'morning' if x < 61 else ('afternoon' if 61 <= x <= 899 else 'evening'))\n",
    "vehiclesDataset.drop(['timestep','id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehiclesDataset['time_of_day'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morning_df = vehiclesDataset[vehiclesDataset['time_of_day'] == 'morning']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afternoon_df = vehiclesDataset[vehiclesDataset['time_of_day'] == 'afternoon']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evening_df = vehiclesDataset[vehiclesDataset['time_of_day'] == 'evening']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morning_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afternoon_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evening_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Učitavanje podataka i računanje statistika\n",
    "\n",
    "stats = evening_df.describe()\n",
    "\n",
    "# Kreiranje grafikona\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "stats.plot(kind='bar', ax=ax)\n",
    "\n",
    "# Podešavanje naslova i oznaka osa\n",
    "ax.set_title('Statistike')\n",
    "ax.set_xlabel('Statistika')\n",
    "ax.set_ylabel('Vrednost')\n",
    "\n",
    "# Prikazivanje grafikona\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prikaz histograma brzine za svaki skup\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.hist(evening_df['speed_kmh'], bins=50, alpha=0.5, label='Evening')\n",
    "ax.hist(afternoon_df['speed_kmh'], bins=50, alpha=0.5, label='Afternoon')\n",
    "\n",
    "# Dodavanje traka za prikaz statističkih mjera za svaki skup\n",
    "evening_stats = evening_df.describe()['speed_kmh']\n",
    "afternoon_stats = afternoon_df.describe()['speed_kmh']\n",
    "ax.axvline(evening_stats['mean'], color='blue', linestyle='--', label='Evening Mean')\n",
    "ax.axvline(afternoon_stats['mean'], color='black', linestyle='--', label='Afternoon Mean')\n",
    "ax.axvline(evening_stats['std']+evening_stats['mean'], color='blue', linestyle=':', label='Evening Std')\n",
    "ax.axvline(afternoon_stats['std']+afternoon_stats['mean'], color='orange', linestyle=':', label='Afternoon Std')\n",
    "ax.axvline(evening_stats['min'], color='blue', linestyle='-.', label='Evening Min')\n",
    "ax.axvline(afternoon_stats['min'], color='black', linestyle='-.', label='Afternoon Min')\n",
    "ax.axvline(evening_stats['max'], color='blue', linestyle='-.', label='Evening Max')\n",
    "ax.axvline(afternoon_stats['max'], color='black', linestyle='-.', label='Afternoon Max')\n",
    "\n",
    "# Postavljanje naziva i legendi za grafikon\n",
    "ax.set_title('Comparison of Speed Distribution and Statistics between Evening and Afternoon')\n",
    "ax.set_xlabel('Speed (km/h)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def prikazi_mere(column):\n",
    "    # Prikaz histograma brzine za svaki skup\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.hist(evening_df[column], bins=50, alpha=0.5, label='Evening')\n",
    "    ax.hist(afternoon_df[column], bins=50, alpha=0.5, label='Afternoon')\n",
    "    ax.hist(morning_df[column], bins=50, alpha=0.5, label='Morning')\n",
    "\n",
    "    # Dodavanje traka za prikaz statističkih mjera za svaki skup\n",
    "    evening_stats = evening_df.describe()[column]\n",
    "    afternoon_stats = afternoon_df.describe()[column]\n",
    "    morning_stats = morning_df.describe()[column]\n",
    "\n",
    "    ax.axvline(evening_stats['mean'], color='blue', linestyle='--', label='Evening Mean')\n",
    "    ax.axvline(afternoon_stats['mean'], color='black', linestyle='--', label='Afternoon Mean')\n",
    "    ax.axvline(morning_stats['mean'], color='green', linestyle='--', label='Morning Mean')\n",
    "\n",
    "    ax.axvline(evening_stats['std']+evening_stats['mean'], color='blue', linestyle=':', label='Evening Std')\n",
    "    ax.axvline(afternoon_stats['std']+afternoon_stats['mean'], color='orange', linestyle=':', label='Afternoon Std')\n",
    "    ax.axvline(morning_stats['std']+morning_stats['mean'], color='purple', linestyle=':', label='Morning Std')\n",
    "\n",
    "    ax.axvline(evening_stats['min'], color='blue', linestyle='-.', label='Evening Min')\n",
    "    ax.axvline(afternoon_stats['min'], color='black', linestyle='-.', label='Afternoon Min')\n",
    "    ax.axvline(morning_stats['min'], color='green', linestyle='-.', label='Morning Min')\n",
    "\n",
    "    ax.axvline(evening_stats['max'], color='blue', linestyle='-.', label='Evening Max')\n",
    "    ax.axvline(afternoon_stats['max'], color='black', linestyle='-.', label='Afternoon Max')\n",
    "    ax.axvline(morning_stats['max'], color='green', linestyle='-.', label='Morning Max')\n",
    "\n",
    "    # Postavljanje naziva i legendi za grafikon\n",
    "    ax.set_title('Comparison of Distribution and Statistics between Evening, Morning and Afternoon')\n",
    "    ax.set_xlabel(column)\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = vehiclesDataset.select_dtypes(include=['int', 'float']).columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_cols:\n",
    "    prikazi_mere(col)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
